import os
from Scripts.readPTU_FLIM import PTUreader
import numpy as np
import pickle

# import pandas as pd
from scipy.signal import savgol_filter

# from focuspoint.correlation_methods.correlation_methods import delayTime2bin
from focuspoint.correlation_methods.correlation_methods import tttr2xfcs


# Possibly more correct to make this a static method of ShrinkFitter class?


# TODO: change mentions of fitter to processing or similar throughout
def loader(filename):
    """
    Loads TCSPC data from PTU file.

    Parameters:
    - filename (str): The name of the file to be loaded.

    Returns:
    - dTimeArr (np.ndarray): The time array from PTU file.
    - trueTimeArr (np.ndarray): The true time (macro-time) in ns.
    - subChanArr (np.ndarray): The sub-channel array from PTU file.
    - micro_res (float): The micro resolution calculated using the PTU file.

    """
    ptu_file = PTUreader(filename, print_header_data=False)
    macro_res = ptu_file.head["MeasDesc_GlobalResolution"] * 1e9
    micro_res = ptu_file.head["MeasDesc_Resolution"] * 1e9

    dTimeArr = ptu_file.tcspc
    trueTimeArr = ptu_file.sync * macro_res
    subChanArr = ptu_file.channel

    return dTimeArr, trueTimeArr, subChanArr, micro_res


def load_decays(df):
    """
    Load TCSPC decays from a DataFrame of filepaths to PTU files

    Parameters:
    df (DataFrame): The DataFrame containing the decay paths.

    Returns:
    int_cps (array): The intensity counts per second.
    int_times (array): The intensity time series.
    decays (array): Array of TCSPC histograms.
    decay_times (array): The decay times (ns), adjusted for micro resolution.
    """
    int_cps = []
    int_times = []
    decays = []
    decay_times = []
    for path in df.paths:
        print(path)
        dTimeArr, trueTimeArr, subChanArr, micro_res = loader(path)
        counts, timeSeries, bins = delayTime2bin(
            trueTimeArr / 1e9, subChanArr, 1, 1)
        decay, time, _ = delayTime2bin(dTimeArr, subChanArr, 1, 1)
        int_cps.append(counts)
        int_times.append(timeSeries)
        decays.append(decay)
        decay_times.append(time)

    int_cps = arrays_me(int_cps)
    int_times = arrays_me(int_times)
    decays = arrays_me(decays)
    # decays = decays - decays[0:150,:].mean(axis=0)
    decay_times = arrays_me(decay_times) * micro_res

    return int_cps, int_times, decays, decay_times


def arrays_me(a):
    """
    Generates a new array by padding the input array `a` with `None` values. The padding is done in such a way that all arrays in the input array have the same length.

    Parameters:
    a (list): The input array containing sub-arrays of varying lengths.

    Returns:
    numpy.ndarray: The new array generated by padding the input array.
    """
    lngth = max(map(len, a))
    y = np.array([xi + [None] * (lngth - len(xi))
                 for xi in a], dtype=float).transpose()
    return y


def batch_class_chop(
    df,
    xstart,
    xend,
    nsteps,
    n_slice,
    NcascStart=5,
    NcascEnd=25,
    Nsub=6,
    smooth=False,
    bck=False,
    bck_bin=150,
    folder="batch",
    inds=None,
    window=None,
    split=10,
):
    """
    Loop through files, process shrinking gate FCS, store and close

    Parameters:
        df (pandas.DataFrame): DataFrame containing the file paths and identifiers.
        xstart (int): First gate for shrinking gate (TCSPC bins)
        xend (int): Last gate for shrinking gate (TCSPC bins)
        nsteps (int): The number of steps between first and last gate in sgFCS
        n_slice (int): The number of time slices to split original data into
        NcascStart (int, optional): The number of logarithmic ranges to skip before calculating the correlation function. Defaults to 5.
        NcascEnd (int, optional): Highest number of logarithmic ranges for calculating the correlation function. Defaults to 25.
        Nsub (int, optional): Number of sublevels per logarithmic range. Defaults to 6.
        smooth (bool, optional): Whether to apply equalise average intensity and remove intensity spikes. Defaults to False.
        bck (bool, optional): Whether to apply background filter. Defaults to False.
        bck_bin (int, optional): Sets last bin considered background. Defaults to 150.
        folder (str, optional): The folder name to store output. Defaults to 'batch'.
        inds (list, optional): The indices of files in df to process. If None, processes all files in df. Defaults to None.
        window (tuple, optional): If not None, supply a time window in s from to analyse. Defaults to None.
        split (int, optional): Criterion for including subintervals based on difference relative to mean. Defaults to 10.

    Returns:
        None
    """
    # Loop through files, process shrinking gate, and close
    if inds is None:
        inds = range(df.shape[0])
    for ind in inds:
        dTimeArr, trueTimeArr, subChanArr, micro_res = loader(df.paths[ind])
        decay, decay_time, _ = delayTime2bin(dTimeArr, subChanArr, 1, 1)

        print(df.pickle_name[ind])

        comb = None
        if smooth:
            comb = count_filter(trueTimeArr, subChanArr, 1.1, smooth=smooth)

        if bck:
            suffix = "_bck.bin"
        else:
            suffix = ".bin"
        # TODO: add in NcascStart,NcascEnd, and Nsub
        fitter = ShrinkFitter(
            df.paths[ind],
            xstart=xstart,
            xend=xend,
            nsteps=nsteps,
            micro_res=micro_res,
            decay=decay,
            decay_time=decay_time,
            bckg=bck,
            bck_bin=bck_bin,
        )

        if window is not None:
            index = np.where(np.logical_and(trueTimeArr / 1e9 >
                             window[0], trueTimeArr / 1e9 < window[1]))
            subChanArr = subChanArr[index]
            trueTimeArr = trueTimeArr[index]
            dTimeArr = dTimeArr[index]
            suffix = "_wind" + suffix
        fitter.shrinkChop(
            subChanArr, trueTimeArr, dTimeArr, n_slice, comb_filt=comb, split=split
        )

        if not os.path.exists(folder + "/"):
            os.makedirs(folder + "/")
        with open(folder + "/" + df.pickle_name[ind] + str(split) + suffix, "wb") as f:
            pickle.dump(fitter, f)


def batch_class_revshrinkchop(
    df,
    xstart,
    xend,
    nsteps,
    n_slice,
    NcascStart=5,
    NcascEnd=25,
    Nsub=6,
    smooth=False,
    bck=False,
    bck_bin=150,
    folder="batch",
    inds=None,
    split=10,
):
    """
    Loop through files, process reverse-shrinking gate FCS (i.e. from first bin up to gate time), store and close

    Parameters:
        df (pandas.DataFrame): DataFrame containing the file paths and identifiers.
        xstart (int): First gate for rev-shrinking gate (TCSPC bins)
        xend (int): Last gate for rev-shrinking gate (TCSPC bins)
        nsteps (int): The number of steps between first and last gate in sgFCS
        n_slice (int): The number of time slices to split original data into
        NcascStart (int, optional): The number of logarithmic ranges to skip before calculating the correlation function. Defaults to 5.
        NcascEnd (int, optional): Highest number of logarithmic ranges for calculating the correlation function. Defaults to 25.
        Nsub (int, optional): Number of sublevels per logarithmic range. Defaults to 6.
        smooth (bool, optional): Whether to apply equalise average intensity and remove intensity spikes. Defaults to False.
        bck (bool, optional): Whether to apply background filter. Defaults to False.
        bck_bin (int, optional): Sets last bin considered background. Defaults to 150.
        folder (str, optional): The folder name to store output. Defaults to 'batch'.
        inds (list, optional): The indices of files in df to process. If None, processes all files in df. Defaults to None.
        window (tuple, optional): If not None, supply a time window in s from to analyse. Defaults to None.
        split (int, optional): Criterion for including subintervals based on difference relative to mean. Defaults to 10.

    Returns:
    None
    """

    if inds is None:
        inds = range(df.shape[0])
    for ind in inds:
        dTimeArr, trueTimeArr, subChanArr, micro_res = loader(df.paths[ind])
        print(df.pickle_name[ind])
        decay, decay_time, _ = delayTime2bin(dTimeArr, subChanArr, 1, 1)

        comb = None
        if smooth:
            comb = count_filter(trueTimeArr, subChanArr, 1.1, smooth=smooth)

        if bck:
            suffix = "_rev_bck.bin"
        else:
            suffix = "_rev.bin"

        # TODO: add in NcascStart,NcascEnd, and Nsub
        fitter = ShrinkFitter(
            df.paths[ind],
            xstart=xstart,
            xend=xend,
            nsteps=nsteps,
            micro_res=micro_res,
            decay=decay,
            decay_time=decay_time,
            bckg=bck,
            bck_bin=bck_bin,
        )

        fitter.revShrinkGateChop(
            subChanArr, trueTimeArr, dTimeArr, n_slice, comb_filt=comb, split=split
        )

        if not os.path.exists(folder + "/"):
            os.makedirs(folder + "/")
        with open(folder + "/" + df.pickle_name[ind] + suffix, "wb") as f:
            pickle.dump(fitter, f)


class ShrinkFitter:
    # TODO: Class description
    # SchrÃ¶der, T et al. PNAS 2023, 120 (4), e2211896120
    def __init__(self, path, xstart, xend, nsteps, micro_res, decay, decay_time, xmax=0, bckg=False, bck_bin=150):
        """
        Initializes an instance of the class with the given parameters.

        Parameters:
            path (str): The path to the file.
            xstart (int): The starting value.
            xend (int): The ending value.
            nsteps (int): The number of shrinking gate steps.
            micro_res (float): The micro resolution in nanoseconds per bin.
            decay (np.ndarray): Histrogram of TCSPC counts
            decay_time (int): TCSPC bins
            xmax (int, optional): The maximum bin value. Defaults to 0.
            bckg (bool, optional): A flag indicating whether background is considered. Defaults to False.
            bck_bin (int, optional): Background will be calculated up to bck_bin. Defaults to 150.

        Returns:
            None
        """
        self.path = path
        self.bckg = bckg
        self.autoCorrs = []
        self.autoCorrsNorm = []
        self.autoTimes = []
        self.autoCorrsStd = []
        self.autoCorrsNormSub = []
        self.autoCorrsStdSub = []
        self.acceptfrac = []
        self.start_gates = []
        self.scale = []
        self.xmax = xmax
        self.xstart = xstart
        self.xend = xend
        self.nsteps = nsteps
        self.NcascStart = 5
        self.NcascEnd = 25
        self.Nsub = 6
        self.micro_res = micro_res
        self.bck_bins = bck_bin
        self.decay = decay
        self.decay_time = decay_time

    def subArrayGeneration(self, dTimeArr, xmin, comb_filt=None):
        """
        Generates a subarray of photon indices and weights based on the given time array.

        Parameters:
            dTimeArr (numpy.ndarray): The array of photon arrival times.
            xmin (float): The minimum time gate for photon arrivals, i.e. the start gate
            comb_filt (numpy.ndarray, optional): User supplied combined filter array. Defaults to None.

        Returns:
            photonInd (numpy.ndarray): The array of photon indices that satisfy the time gate condition.
            phot_wgt (numpy.ndarray): The array of photon weights based on the combined filter array.
        """
        # If max time gate not set, set to last gate recorded - 5 (the last few gates have a systematic reduction in counts)
        if self.xmax == 0:
            self.xmax = np.max(dTimeArr) - 5

        if self.xmax < xmin:
            xmin1 = xmin
            self.xmin = self.xmax
            self.xmax = xmin1

        # Finds those photons which arrive above certain time or below certain micro-time.
        photonInd = np.logical_and(dTimeArr >= xmin, dTimeArr <= self.xmax).astype(
            bool
        )  # is it better to have dtype='bool' in the main function?

        phot_wgt = np.zeros(photonInd.shape)

        # Weights the photons based on the combined filter array if present
        if comb_filt is not None:
            phot_wgt = photonInd * comb_filt

        return photonInd, phot_wgt

    def bckg_filter_gate(self, dTimeArr, subChan, xmin):
        """
        Generate the background-filter for a given start gate. Matrix math from Methods 140-141 (2018) 32-39

        TODO: provide reference to matrix math

        Args:
            dTimeArr (np.array): An array of time delay values.
            subChan (np.array): An array of sub-channel number.
            xmin (int): The minimum value for the gate.

        Returns:
            np.array: An array of background-filtered gate values.
        """
        # TODO: replace photons_in_bin with self.decay and test if it works
        photons_in_bin, _, _ = delayTime2bin(dTimeArr, subChan, 1, 1)
        photons_in_bin = np.array(photons_in_bin, dtype=float)
        # bck = np.mean(photons_in_bin[0:150]) #work out bckground from first 150 bins
        bck = np.mean(
            photons_in_bin[2: self.bck_bins]
        )  # number of bins to average will depend on TCSPC unit (e.g. Multiharp or Hydraharp)

        if self.xmax == 0:
            photons_in_bin_cut = photons_in_bin[
                xmin:-5
            ]  # last couple bins can be trash
        else:
            photons_in_bin_cut = photons_in_bin[xmin: self.xmax]

        bck_mat = np.ones(len(photons_in_bin_cut))
        bck_mat = bck_mat / np.linalg.norm(bck_mat, 1)

        # bck = np.mean(photons_in_bin[-100:-5]) #work out bckground from last 100 bins
        photon_mat = photons_in_bin_cut - bck
        photon_mat = photon_mat / np.linalg.norm(photon_mat, 1)

        photons_in_bin_cut[photons_in_bin_cut < 1] = 1

        diag = np.diag(np.reciprocal(photons_in_bin_cut))
        pat_mat = np.array([bck_mat, photon_mat])
        inv_mat = np.linalg.inv(
            np.linalg.multi_dot([pat_mat, diag, pat_mat.transpose()])
        )
        filt_mat_temp = np.linalg.multi_dot([inv_mat, pat_mat, diag])

        filt_mat = np.zeros((2, len(photons_in_bin)))
        if self.xmax == 0:
            # last couple bins can be trash
            filt_mat[:, xmin:-5] = filt_mat_temp
        else:
            filt_mat[
                :, xmin: self.xmax
            ] = filt_mat_temp  # clunky but not worth optimising for now
        d_time_temp = dTimeArr.copy()
        d_max = d_time_temp.max()
        d_time_temp[d_time_temp == d_max] = d_max - 1
        bck_filt = filt_mat[1, d_time_temp]

        return bck_filt

    def tGate(self, subChan, tTime, dTime, xmin, comb_filt=None):
        """
        Calculate autocorrelation function for a given sub-channel and start time gate. NB speed is RAM limited if subChan is too large

        Args:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            xmin (float): The xmin parameter.
            comb_filt (Optional[np.ndarray]): The comb_filt parameter. Defaults to None.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing the auto, autonorm, and autotime arrays.
        """
        # Grab gated photon indices
        photonInd = self.subArrayGeneration(dTime, xmin, comb_filt=comb_filt)

        num = np.zeros((subChan.shape[0], 2))
        num[:, 0] = photonInd
        # Run Autocorrelation
        auto, autotime = tttr2xfcs(
            tTime, num, self.NcascStart, self.NcascEnd, self.Nsub
        )

        totphotons = np.sum(num[:, 0])
        print(totphotons)
        maxY = np.ceil(np.max(tTime)) - np.floor(np.min(tTime))

        autonorm = np.zeros((auto.shape))

        autonorm[:, 0, 0] = ((auto[:, 0, 0] * maxY) / (totphotons**2)) - 1

        return auto, autonorm, autotime

    def tGateFast(self, subChan, tTime, dTime, xmin, comb_filt=None):
        """
        Calculate gated autocorrelation function. Speeds up the tGate function by only supplying non-zero weighted photons to ttr2xfcs

        Args:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            xmin (float): Bin to start correlation at
            comb_filt (Optional[np.ndarray]): The comb_filt parameter. Defaults to None.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing the auto, autonorm, and autotime arrays.
        """
        # Grab gated photon indices
        photonInd, filt = self.subArrayGeneration(
            dTime, xmin, comb_filt=comb_filt)

        # temporary fix to allow filtering
        photonInd_int = photonInd.copy()  # .astype('int')

        num = np.zeros((subChan.shape[0], 2))

        if comb_filt is not None:
            num[:, 0] = photonInd * comb_filt
        else:
            num[:, 0] = photonInd
        # Run Autocorrelation
        auto, autotime = tttr2xfcs(
            tTime[photonInd],
            num[photonInd, :],
            self.NcascStart,
            self.NcascEnd,
            self.Nsub,
        )

        totphotons = np.sum(num[:, 0])
        print(f"total photons in gate = {totphotons}")
        maxY = np.ceil(np.max(tTime)) - np.floor(np.min(tTime))

        autonorm = np.zeros((auto.shape))

        autonorm[:, 0, 0] = ((auto[:, 0, 0] * maxY) / (totphotons**2)) - 1

        return auto, autonorm, autotime

    # TODO: integrate with tGate with if statement
    def tGate_bck(self, subChan, tTime, dTime, xmin, comb_filt=None):
        """
        Calculate autocorrelation function for a given sub-channel and start time gate with background correction.
        NB speed is RAM limited if subChan is too large

        Args:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            xmin (float): The xmin parameter.
            comb_filt (Optional[np.ndarray]): The comb_filt parameter. Defaults to None.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing the auto, autonorm, and autotime arrays.
        """

        # Grab gated photon indices
        photonInd = self.subArrayGeneration(dTime, xmin, comb_filt=comb_filt)

        # do gated background correction
        photonInd = photonInd * self.bckg_filter_gate(dTime, subChan, xmin)

        num = np.zeros((subChan.shape[0], 2))
        num[:, 0] = photonInd
        # Run Autocorrelation
        auto, autotime = tttr2xfcs(
            tTime, num, self.NcascStart, self.NcascEnd, self.Nsub
        )

        totphotons = np.sum(num[:, 0])
        print(totphotons)
        maxY = np.ceil(np.max(tTime)) - np.floor(np.min(tTime))

        autonorm = np.zeros((auto.shape))

        autonorm[:, 0, 0] = ((auto[:, 0, 0] * maxY) / (totphotons**2)) - 1

        return auto, autonorm, autotime

    # TODO: integrate with tGate_fast with if statement
    def tGate_bckFast(self, subChan, tTime, dTime, xmin, comb_filt=None):
        """
        Calculate gated autocorrelation function with bacgkround correction. Speeds up the tGate function by only supplying non-zero weighted photons to ttr2xfcs

        Args:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            xmin (float): The xmin parameter.
            comb_filt (Optional[np.ndarray]): The comb_filt parameter. Defaults to None.

        Returns:
            Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing the auto, autonorm, and autotime arrays.
        """
        # Grab gated photon indices
        photonInd, _ = self.subArrayGeneration(
            dTime, xmin, comb_filt=comb_filt)

        # do gated background correction
        # photonInd = photonInd * self.bckg_filter_gate(dTime,subChan,xmin)

        num = np.zeros((subChan.shape[0], 2))
        num[:, 0] = photonInd * self.bckg_filter_gate(dTime, subChan, xmin)
        # Run Autocorrelation
        auto, autotime = tttr2xfcs(
            tTime[photonInd],
            num[photonInd, :],
            self.NcascStart,
            self.NcascEnd,
            self.Nsub,
        )

        # totphotons = np.count_nonzero(num[:,0]) #changed from np.sum because bckground filter values can differ from 1 - DUMB idea
        totphotons = np.sum(num[:, 0])
        print(totphotons)
        maxY = np.ceil(np.max(tTime)) - np.floor(np.min(tTime))

        autonorm = np.zeros((auto.shape))

        autonorm[:, 0, 0] = ((auto[:, 0, 0] * maxY) / (totphotons**2)) - 1

        return auto, autonorm, autotime

    def shrinkGate(self, subChan, tTime, dTime, comb_filt=None):
        """
        Perform and store shrinking gate fluorescence correlation spectroscopy. Large files will be slower than shrinkChop due to RAM usage.

        Parameters:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            comb_filt (None or array-like, optional): The filter for the combined channels. Defaults to None.

        Returns:
            None
        """
        # Initialise arrays
        autoTimes = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps))
        autoCorrs = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsNorm = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        self.start_gates = np.floor(
            np.linspace(self.xstart, self.xend, num=self.nsteps)
        ).astype(int)
        # Run and store gated autocorrelations

        for i in range(self.nsteps):
            if self.bckg:
                # currently monkey patching bckground correction and could be handled better
                auto, autonorm, autotime = self.tGate_bckFast(
                    subChan, tTime, dTime, self.start_gates[i], comb_filt=comb_filt
                )
            else:
                print("No bckground")
                auto, autonorm, autotime = self.tGateFast(
                    subChan, tTime, dTime, self.start_gates[i], comb_filt=comb_filt
                )
            length = len(autotime)
            autoCorrs[:length, i] = auto[:, 0, 0]
            autoTimes[:length, i] = autotime[:, 0]
            autoCorrsNorm[:length, i] = autonorm[:, 0, 0]

        self.autoCorrs = autoCorrs
        self.autoCorrsNorm = autoCorrsNorm
        self.autoTimes = autoTimes
        self.autoTimes[autoTimes == 0] = np.nan

    def revShrinkGate(self, subChan, tTime, dTime, comb_filt=None):
        """
        Perform and store reverse shrinking gate fluorescence correlation spectroscopy where gate is from first (5th in practice) up to specified gate bin

        Parameters:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            comb_filt (None or array-like, optional): The filter for the combined channels. Defaults to None.

        Returns:
            None
        """
        # Initialise arrays
        autoTimes = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps))
        autoCorrs = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsNorm = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        self.end_gates = np.floor(
            np.linspace(self.xstart, self.xend, num=self.nsteps)
        ).astype(int)
        # Run and store gated autocorrelations

        for i in range(self.nsteps):
            self.xmax = self.end_gates[i]
            auto, autonorm, autotime = self.tGateFast(
                subChan, tTime, dTime, xmin=0, comb_filt=comb_filt
            )
            length = len(autotime)
            autoCorrs[:length, i] = auto[:, 0, 0]
            autoTimes[:length, i] = autotime[:, 0]
            autoCorrsNorm[:length, i] = autonorm[:, 0, 0]

        self.autoCorrs = autoCorrs
        self.autoCorrsNorm = autoCorrsNorm
        self.autoTimes = autoTimes
        self.autoTimes[autoTimes == 0] = np.nan

    def crossFit(self, dTimeArr, tTimeArr, subChan, x_in1, x_end1, x_in2):
        """
        Calculate the cross-correlation for the given input data. This is should work but is old code. See tttr2xfcs documentation to understand how it works

        Parameters:


        Returns:

        """

        x_max = np.max(dTimeArr) - 5
        reg1 = np.logical_and(
            dTimeArr >= x_in1, dTimeArr <= x_end1).astype(bool)
        reg2 = np.logical_and(dTimeArr > x_in2, dTimeArr <= x_max).astype(bool)

        num = np.zeros((subChan.shape[0], 2))
        num[:, 0] = reg1
        num[:, 1] = reg2

        # Run Autocorrelation
        auto, auto_time = tttr2xfcs(
            tTimeArr, num, self.NcascStart, self.NcascEnd, self.Nsub
        )
        totphotons = np.sum(num, axis=0)

        maxY = np.ceil(np.max(tTimeArr))
        auto_norm = np.zeros((auto.shape))

        auto_norm[:, 0, 0] = ((auto[:, 0, 0] * maxY) /
                              (totphotons[0] ** 2)) - 1

        auto_norm[:, 1, 1] = (
            (auto[:, 1, 1] * maxY) / (totphotons[1] * totphotons[1])
        ) - 1
        auto_norm[:, 1, 0] = (
            (auto[:, 1, 0] * maxY) / (totphotons[1] * totphotons[0])
        ) - 1
        auto_norm[:, 0, 1] = (
            (auto[:, 0, 1] * maxY) / (totphotons[0] * totphotons[1])
        ) - 1

        return auto, auto_norm, auto_time

    def chopper(
        self, subChan, tTime, dTime, n_slice, start_gate, comb_filt=None, split=100
    ):
        """
        Performs time gated FCS by splitting the data into n_slice even time intervals. Average deviation of each curve is compared.
        Curves that deviate by more than the split ratio are rejected. To ensure a reasonable number of curves, a minimum of 5 curves are accepted.
        Alogorithm is an impferect implementation of the algorithm in Ries et al. , Optics Express 2010, 18 (11), 11073



        Args:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            xmin (float): The xmin parameter.
            n_slice (int): The number of even time slices.
            start_gate (int): The start gate/bin for gated gated FCS.
            comb_filt (numpy.ndarray, optional): The comb_filt parameter. Defaults to None.
            split (float, optional): Ratio of average deviation to accept. Defaults to 100.

        Returns:
            tuple: A tuple containing the following elements:
                - autoTimes (numpy.ndarray): Array of autotimes for each slice
                - autoCorrsNorm (numpy.ndarray): Array of autoCorrsNorm for each slice
                - meanTime (numpy.ndarray): The meanTime array for all curves.
                - meanCorr (numpy.ndarray): The meanCorr array for all curves.
                - stdevCorr (numpy.ndarray): The stdevCorr array for all curves.
                - meanCorrSub (numpy.ndarray): The meanCorrSub array for accepted curves.
                - stdevCorrSub (numpy.ndarray): The stdevCorrSub array for accepted curves.
                - accept_frac (float): The accept_frac value.
        """

        slice_size = np.ceil(np.max(tTime) / 1e9 / n_slice)
        autoTimes = np.zeros(((self.NcascEnd + 1) * (self.Nsub + 1), n_slice))
        autoCorrs = np.zeros(((self.NcascEnd + 1) * (self.Nsub + 1), n_slice)).astype(
            np.float64
        )
        autoCorrsNorm = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), n_slice)
        ).astype(np.float64)

        for i in range(n_slice):
            index = np.where(
                np.logical_and(
                    tTime / 1e9 > i * slice_size, tTime /
                    1e9 < (i + 1) * slice_size
                )
            )
            # print(np.min(index))
            # print(np.max(index))
            if self.bckg:
                auto, autonorm, autotime = self.tGate_bckFast(
                    subChan[index], tTime[index], dTime[index], start_gate
                )
            elif comb_filt is not None:
                auto, autonorm, autotime = self.tGateFast(
                    subChan[index],
                    tTime[index],
                    dTime[index],
                    start_gate,
                    comb_filt=comb_filt[index],
                )
            else:
                auto, autonorm, autotime = self.tGateFast(
                    subChan[index],
                    tTime[index],
                    dTime[index],
                    start_gate,
                    comb_filt=comb_filt,
                )

            length = len(autotime)
            autoCorrs[:length, i] = auto[:, 0, 0]
            autoTimes[:length, i] = autotime[:, 0]
            autoCorrsNorm[:length, i] = autonorm[:, 0, 0]

        autoTimes[autoTimes == 0] = np.nan

        meanTime = np.mean(autoTimes[:, :-1], axis=1)
        meanCorr = np.mean(autoCorrsNorm[:, :-1], axis=1)
        stdevCorr = np.std(autoCorrsNorm[:, :-1], axis=1)

        diff = np.zeros(n_slice)
        slices = np.arange(n_slice)
        for row in slices:
            row_av = autoCorrsNorm[40:, row].mean()
            diff[row] = (row_av - autoCorrsNorm[40:,
                         slices[slice != row]].mean()) ** 2

        accept = np.where(diff / diff.min() < split)[0]
        diff_temp = diff.copy()
        # TODO: make the fraction of curves accepted an input
        while len(accept) < n_slice/2:  # Arbitrayily want a minimum of 50 pct of curves to get stdev
            print("Less than half accepted. Normalising by next closest curve")
            diff_temp = np.delete(diff_temp, np.argmin(diff_temp))
            accept = np.where(diff / diff_temp.min() < split)[0]
        meanCorrSub = autoCorrsNorm[:, accept].mean(axis=1)
        stdevCorrSub = np.std(autoCorrsNorm[:, accept], axis=1)
        accept_frac = len(accept) / n_slice
        return (
            autoTimes,
            autoCorrsNorm,
            meanTime,
            meanCorr,
            stdevCorr,
            meanCorrSub,
            stdevCorrSub,
            accept_frac,
        )

    def scaler(self, start_gate):
        """
        Returns scale factor to take into account uncorrelated background effect on autocorrelation amplitude

        See Sup info from SchrÃ¶der, T et al. PNAS 2023, 120 (4), e2211896120

        Parameters:
            start_gate (int): The index of the start gate in the decay array.

        Returns:
            float: The calculated scaling value.

        """
        bck = np.array(self.decay[5: self.bck_bins]).mean()
        I_gate_mean = np.array(self.decay[start_gate:-5]).mean()
        return (I_gate_mean / (I_gate_mean - bck)) ** 2

    def shrinkChop(self, subChan, tTime, dTime, n_slice, comb_filt=None, split=100):
        """
        Perform and store shrinking gate fluorescence correlation spectroscopy with chopping alogorithm

        Parameters:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            comb_filt (None or array-like, optional): The filter for the combined channels. Defaults to None.
            split (float, optional): Ratio of average deviation to accept. Defaults to 100.

        Returns:
            None
        """
        # Initialise arrays
        autoTimes = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps))
        autoCorrs = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsNorm = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsStd = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsNormSub = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsStdSub = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        acceptfrac = np.zeros(self.nsteps).astype(np.float64)
        scales = np.zeros(self.nsteps).astype(np.float64)

        self.start_gates = np.floor(
            np.linspace(self.xstart, self.xend, num=self.nsteps)
        ).astype(int)
        # Run and store gated autocorrelations

        for i in range(self.nsteps):
            print("Choppin")
            (
                _,
                _,
                meanTime,
                meanCorr,
                stdevCorr,
                meanCorrSub,
                stdevCorrSub,
                accept_frac,
            ) = self.chopper(
                subChan,
                tTime,
                dTime,
                n_slice,
                self.start_gates[i],
                comb_filt=comb_filt,
                split=split,
            )

            scale = self.scaler(self.start_gates[i])

            length = len(meanTime)
            # autoCorrs[:length,i] = auto[:,0,0]
            autoTimes[:length, i] = meanTime[:]
            autoCorrsNorm[:length, i] = meanCorr[:]
            autoCorrsStd[:length, i] = stdevCorr
            autoCorrsNormSub[:length, i] = meanCorrSub[:]
            autoCorrsStdSub[:length, i] = stdevCorrSub
            acceptfrac[i] = accept_frac
            scales[i] = scale

        # self.autoCorrs = autoCorrs
        self.autoCorrsNorm = autoCorrsNorm
        self.autoTimes = autoTimes
        self.autoTimes[autoTimes == 0] = np.nan
        self.autoCorrsStd = autoCorrsStd
        self.autoCorrsNormSub = autoCorrsNormSub
        self.autoCorrsStdSub = autoCorrsStdSub
        self.acceptfrac = acceptfrac
        self.scales = scales

    def revShrinkGateChop(
        self, subChan, tTime, dTime, n_slice, comb_filt=None, split=10
    ):
        """
        Perform and store reverse shrinking gate fluorescence correlation spectroscopy with chopping alogorithm

        Parameters:
            subChan (np.ndarray): Array of photon channels
            tTime (np.ndarray): Array of macro-/true- arrival times (ns)
            dTime (np.ndarray): Array of micro-arrival TCSPC bins
            comb_filt (None or array-like, optional): The filter for the combined channels. Defaults to None.
            split (float, optional): Ratio of average deviation to accept. Defaults to 100.

        Returns:
            None
        """
        # Initialise arrays
        autoTimes = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps))
        autoCorrs = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsNorm = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsStd = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsNormSub = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        autoCorrsStdSub = np.zeros(
            ((self.NcascEnd + 1) * (self.Nsub + 1), self.nsteps)
        ).astype(np.float64)
        acceptfrac = np.zeros(self.nsteps).astype(np.float64)

        self.end_gates = np.floor(
            np.linspace(self.xstart, self.xend, num=self.nsteps)
        ).astype(int)
        # Run and store gated autocorrelations

        for i in range(self.nsteps):
            self.xmax = self.end_gates[i]
            # auto,autonorm, autotime = self.tGateFast(subChan,tTime,dTime,xmin=0,comb_filt=comb_filt)
            (
                _,
                _,
                meanTime,
                meanCorr,
                stdevCorr,
                meanCorrSub,
                stdevCorrSub,
                accept_frac,
            ) = self.chopper(
                subChan,
                tTime,
                dTime,
                n_slice,
                start_gate=0,
                comb_filt=comb_filt,
                split=split,
            )
            length = len(meanTime)
            # autoCorrs[:length,i] = auto[:,0,0]
            autoTimes[:length, i] = meanTime[:]
            autoCorrsNorm[:length, i] = meanCorr[:]
            autoCorrsStd[:length, i] = stdevCorr
            autoCorrsNormSub[:length, i] = meanCorrSub[:]
            autoCorrsStdSub[:length, i] = stdevCorrSub
            acceptfrac[i] = accept_frac

        # self.autoCorrs = autoCorrs
        self.autoCorrsNorm = autoCorrsNorm
        self.autoTimes = autoTimes
        self.autoTimes[autoTimes == 0] = np.nan
        self.autoCorrsStd = autoCorrsStd
        self.autoCorrsNormSub = autoCorrsNormSub
        self.autoCorrsStdSub = autoCorrsStdSub
        self.acceptfrac = acceptfrac


def delayTime2bin(dTimeArr, chanArr, chanNum, winInt):
    """
    Calculate TCSPC histogram or intensity trace if micro-time (dTime) or macro-time (tTime) is supplied

    Parameters:
        - dTimeArr (np.ndarray): The time array from PTU file. If true time / 1e9 is used it will calculate the intensity trace
        - chanArr (np.ndarray): The sub-channel array from PTU file.
    - chanNum (int): The channel number of the sub-channel array
    - winInt (float): Coarsening factor for the binning.
    Returns:
        - photonsInBin (list): List of the number of photons in each bin
        - decayScale (list): List of the decay scale values.
        - bins (np.ndarray): The bins array.
    """
    decayTime = np.array(dTimeArr)
    # This is the point and which each channel is identified.
    decayTimeCh = decayTime[chanArr == chanNum]

    # Find the first and last entry
    # firstDecayTime = 0;#np.min(decayTimeCh).astype(np.int32)
    firstDecayTime = np.min(decayTimeCh).astype(np.int32)
    tempLastDecayTime = np.max(decayTimeCh).astype(np.int32)

    # We floor this as the last bin is always incomplete and so we discard photons.
    numBins = np.floor((tempLastDecayTime - firstDecayTime) / winInt)
    lastDecayTime = numBins * winInt

    bins = np.linspace(firstDecayTime, lastDecayTime, int(numBins) + 1)

    # TODO - at the moment photons from the last two bins are added together because RHS of the last bin is closed (open for the rest)

    photonsInBin, _ = np.histogram(decayTimeCh, bins)

    # bins are valued as half their span.
    decayScale = bins[:-1] + (winInt / 2)

    # decayScale =  np.arange(0,decayTimeCh.shape[0])

    return list(photonsInBin), list(decayScale), bins


def count_filter(tTime, subChan, high_cut, smooth=False):
    """
    Count filter function that identifies the high intensity events in the TCSPC and optionally smooths/normalises long term changes in intensity

    Parameters:
        tTime (np.ndarray): Array of macro-/true- arrival times (ns)
        subChan (np.ndarray): Array of photon channels
        high_cut (float): The high cut ratio relative to intensity at t=0
        smooth (bool, optional): Whether to apply smoothing or not. Defaults to False.

    Returns:
        tuple: A tuple containing the filtered counts and the count weights.
    """
    counts, _, bins = delayTime2bin(tTime / 1e9, subChan, 1, 1)
    tTime_binned = np.digitize(np.array(tTime / 1e9), bins)

    savgol = savgol_filter(counts, 50, 3)
    init = np.mean(
        counts[0:30]
    )  # assume true countrate is the average of the first 30 s
    c_weight = 1 / np.sqrt(savgol / init) + (1 - np.sqrt(savgol / init))

    bin_comp = bins[1:]  # match length of tTimebinned (bins start at 1 not 0?)

    filt = np.in1d(
        tTime_binned, bin_comp[np.array(
            counts * c_weight) / counts[0] < high_cut]
    )

    if smooth:
        smoother = np.zeros(len(tTime))
        smoother[tTime_binned < tTime_binned[-1]] = c_weight[
            tTime_binned[tTime_binned < tTime_binned[-1]] - 1
        ]
        filt = filt * smoother

    return filt, c_weight


def time_filter(tTime, subChan, time_start, time_end):
    """
    Filters the given time values based on the specified time range.

    Args:
        tTime (np.ndarray): Array of macro-/true- arrival times (ns)
       subChan (np.ndarray): Array of photon channels
        time_start (float): Start time of  window (s)
        time_end (float): End time of  window (s)

    Returns:
        filt (np.ndarray): An array of boolean values indicating whether each time value falls within the specified range.
    """
    counts, _, bins = delayTime2bin(tTime / 1e9, subChan, 1, 1)
    tTime_binned = np.digitize(np.array(tTime / 1e9), bins)

    # data in first bin is 0 indexed
    bin_comp = bins[time_start + 1: time_end + 1]

    filt = np.in1d(tTime_binned, bin_comp)

    return filt

def bckg_filter(dTimeArr,subChan,bck_max):
    photons_in_bin,_,_ = delayTime2bin(dTimeArr, subChan, 1, 1)
    photons_in_bin = np.array(photons_in_bin,dtype=float)
    bck_mat = np.ones(len(photons_in_bin))
    bck_mat = bck_mat / np.linalg.norm(bck_mat,1)
    
    bck = np.mean(photons_in_bin[2:bck_max])
    photon_mat = photons_in_bin - bck
    photon_mat = photon_mat / np.linalg.norm(photon_mat,1)

    photons_in_bin[photons_in_bin < 1] = 1 #Prevent reciprocal from breaking if there are no photons in a bin
    diag = np.diag(np.reciprocal(photons_in_bin))
    pat_mat = np.array([bck_mat,photon_mat])
    inv_mat = np.linalg.inv(np.linalg.multi_dot([pat_mat,diag,pat_mat.transpose()]))
    filt_mat = np.linalg.multi_dot([inv_mat,pat_mat,diag])

    d_time_temp = dTimeArr.copy()
    d_max = d_time_temp.max()
    d_time_temp[d_time_temp == d_max] = d_max - 1
    bck_filt = filt_mat[1,d_time_temp]
    
    return filt_mat, bck_filt  